# -*- coding: utf-8 -*-
"""Copy of attebtion for time-series

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UKgJJL3o6NMQkrttEe1357X1iGcy-OzC

Basic Attention core functions for time-series prediction
"""

import tensorflow as tf
import numpy as np
import os
import shutil

tf.compat.v1.disable_eager_execution()

class Attention():
    def __init__(self, model_parameters):
        tf.compat.v1.reset_default_graph()
        self.task = model_parameters['task']
        self.h_dim = model_parameters['h_dim']
        self.batch_size = model_parameters['batch_size']
        self.epoch = model_parameters['epoch']
        self.learning_rate = model_parameters['learning_rate']
        self.save_file_directory = 'tmp/attention/'

    def process_batch_input_for_RNN(self, batch_input):
        # batch_input: (B, T, x_dim) -> processed (T, B, x_dim)
        return tf.transpose(batch_input, perm=[1, 0, 2])

    def sample_X(self, m, n):
        return np.random.permutation(m)[:n]

    def fit(self, x, y):
        # x: numpy (no, seq_len, x_dim)
        # y: numpy (no, y_dim)
        no, seq_len, x_dim = x.shape
        y_dim = y.shape[1]

        initializer = tf.random.truncated_normal_initializer(mean=0.0, stddev=0.01)

        # --- GRU weights ---
        Wr = tf.Variable(initializer([x_dim, self.h_dim]), name='Wr')
        Ur = tf.Variable(initializer([self.h_dim, self.h_dim]), name='Ur')
        br = tf.Variable(tf.zeros([self.h_dim]), name='br')

        Wu = tf.Variable(initializer([x_dim, self.h_dim]), name='Wu')
        Uu = tf.Variable(initializer([self.h_dim, self.h_dim]), name='Uu')
        bu = tf.Variable(tf.zeros([self.h_dim]), name='bu')

        Wh_gru = tf.Variable(initializer([x_dim, self.h_dim]), name='Wh_gru')
        Uh = tf.Variable(initializer([self.h_dim, self.h_dim]), name='Uh')
        bh = tf.Variable(tf.zeros([self.h_dim]), name='bh')

        # --- Attention weights (Bahdanau) ---
        # project h_t -> d, project s_T -> d, then v -> 1
        attn_dim = self.h_dim  # you can set different attention dimension
        Wa_h = tf.Variable(initializer([self.h_dim, attn_dim]), name='Wa_h')   # W_h
        Wa_s = tf.Variable(initializer([self.h_dim, attn_dim]), name='Wa_s')   # W_s
        ba = tf.Variable(tf.zeros([attn_dim]), name='ba')
        Va = tf.Variable(initializer([attn_dim, 1]), name='Va')  # v

        # Output weights
        Wo = tf.Variable(initializer([self.h_dim, y_dim]), name='Wo')
        bo = tf.Variable(tf.zeros([y_dim]), name='bo')

        # Placeholders
        Y = tf.compat.v1.placeholder(tf.float32, [None, y_dim], name='Y')               # (B, y_dim)
        _inputs = tf.compat.v1.placeholder(tf.float32, [None, None, x_dim], name='inputs')  # (B, T, x_dim)

        # processed input: (T, B, x_dim)
        processed_input = self.process_batch_input_for_RNN(_inputs)

        # initial hidden: zeros (B, H)
        batch_size_dynamic = tf.shape(_inputs)[0]
        initial_hidden = tf.zeros([batch_size_dynamic, self.h_dim], dtype=tf.float32)

        # --- GRU cell function ---
        def GRU(prev_h, x_t):
            # prev_h: (B, H), x_t: (B, x_dim)
            r = tf.sigmoid(tf.matmul(x_t, Wr) + tf.matmul(prev_h, Ur) + br)
            u = tf.sigmoid(tf.matmul(x_t, Wu) + tf.matmul(prev_h, Uu) + bu)
            c = tf.tanh(tf.matmul(x_t, Wh_gru) + tf.matmul(tf.multiply(r, prev_h), Uh) + bh)
            h_t = tf.multiply(1.0 - u, prev_h) + tf.multiply(u, c)
            return h_t

        # all_hidden_states: (T, B, H)
        all_hidden_states = tf.scan(fn=GRU, elems=processed_input, initializer=initial_hidden, name='states')

        # --- Attention computation (vectorized) ---
        # all_hidden_states: (T, B, H)
        # s_T = last hidden state: (B, H)
        s_T = all_hidden_states[-1]  # (B, H)

        # W_h * h_t  -> do tensordot over last dim: result (T, B, attn_dim)
        Wh_ht = tf.tensordot(all_hidden_states, Wa_h, axes=[[2], [0]])  # (T, B, attn_dim)
        # W_s * s_T -> (B, attn_dim), expand to (T, B, attn_dim)
        Ws_sT = tf.matmul(s_T, Wa_s)  # (B, attn_dim)
        Ws_sT_exp = tf.expand_dims(Ws_sT, axis=0)  # (1, B, attn_dim)
        Ws_sT_tiled = tf.tile(Ws_sT_exp, [tf.shape(Wh_ht)[0], 1, 1])  # (T, B, attn_dim)

        # score: (T, B, attn_dim)
        score = tf.nn.tanh(Wh_ht + Ws_sT_tiled + ba)

        # e_values: (T, B, 1)
        e_values = tf.tensordot(score, Va, axes=[[2], [0]])  # (T, B, 1)

        # attention weights alpha: softmax over time (axis=0)
        a_values = tf.nn.softmax(e_values, axis=0)  # (T, B, 1)

        # context vector: sum_t alpha_t * h_t -> (B, H)
        context_vector = tf.reduce_sum(a_values * all_hidden_states, axis=0)  # (B, H)

        # output (logits) and prediction
        logits = tf.matmul(context_vector, Wo) + bo  # (B, y_dim)
        outputs = tf.identity(tf.nn.sigmoid(logits), name='outputs')  # ensure name 'outputs'

        # --- Loss ---
        if self.task == 'classification':
            loss = -tf.reduce_mean(Y * tf.math.log(outputs + 1e-8) + (1 - Y) * tf.math.log(1 - outputs + 1e-8))
        else:
            loss = tf.sqrt(tf.reduce_mean(tf.square(outputs - Y)))

        # Optimizer & train op
        optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate)
        train_step = optimizer.minimize(loss)

        # Session
        sess = tf.compat.v1.Session()
        sess.run(tf.compat.v1.global_variables_initializer())

        iteration_per_epoch = int(no / self.batch_size)
        iterations = self.epoch * iteration_per_epoch

        for i in range(iterations):
            idx = self.sample_X(no, self.batch_size)
            batch_x = x[idx, :, :]   # numpy indexing
            batch_y = y[idx, :]

            _, step_loss = sess.run([train_step, loss], feed_dict={_inputs: batch_x, Y: batch_y})

            if i % iteration_per_epoch == (iteration_per_epoch - 1):
                print(f"Epoch {i // iteration_per_epoch}, Loss {step_loss:.6f}")

        # Save model (make directory fresh)
        if os.path.exists(self.save_file_directory):
            shutil.rmtree(self.save_file_directory)
        os.makedirs(self.save_file_directory, exist_ok=True)

        inputs_map = {'inputs': _inputs}
        outputs_map = {'outputs': outputs}
        tf.compat.v1.saved_model.simple_save(sess, self.save_file_directory, inputs_map, outputs_map)

        # Keep session open? we close here
        sess.close()

    def predict(self, test_x):
        """
        Load saved model and predict.
        test_x: numpy array (N, T, x_dim)
        """
        graph = tf.Graph()
        with graph.as_default():
            with tf.compat.v1.Session() as sess:
                tf.compat.v1.saved_model.loader.load(sess, [tf.saved_model.SERVING], self.save_file_directory)
                x = graph.get_tensor_by_name('inputs:0')
                outputs = graph.get_tensor_by_name('outputs:0')
                test_y_hat = sess.run(outputs, feed_dict={x: test_x})
        return test_y_hat